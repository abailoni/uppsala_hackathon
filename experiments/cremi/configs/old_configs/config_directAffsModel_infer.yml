
device: cuda

loaders:
  infer:
    inference_mode: True
    loader_config:
      # Number of processes to use for loading data. Set to (say) 10 if you wish to
      # use 10 CPU cores, or to 0 if you wish to use the same process for training and
      # data-loading (generally not recommended).
      batch_size: 1
      num_workers: 10
      drop_last: False
#      pin_memory: False
      shuffle: False
    name: C

    master_config:
      downscale_and_crop:
        replicate_targets: True
        0:
          ds_factor: [1, 3, 3]
          crop_factor: [1, 1, 1]
        1:
          ds_factor: [1, 1, 1]
          crop_factor: [1, 2, 2]
          crop_slice: "3:-3, : , :"

    volume_config:
      # Sliding window size
      window_size: &shape [20, 756, 756]  # 288
#        A:
#        B: *shape
#        C: *shape
      # Sliding window stride
      stride: &stride [6, 256, 256] # This is usually equal to the prediction shape, after being cropped
#        A:
#        B: *stride
#        C: *stride
      # Data slice to iterate over.
      data_slice: ':, :, :'
#        A: ':20, :, :'
#        B: ':, :, :'
#        C: ':, :, :'

#      padding_mode: "constant"
      padding: # As compared to the input, how much should I crop (global crop)
        - [7, 7]
        - [250, 250]
        - [250, 250]
      # PATHS:
      path:
        A: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleA_train.h5'
        B: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleB_train.h5'
        C: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleC_train.h5'
        B+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleB+_cropped_plus_mask.h5'
        A+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleA+_cropped_plus_mask.h5'
        C+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleC+_cropped_plus_mask.h5'
      path_in_file:
        A: 'raw'
        B: 'raw_old'
        C: 'raw'
        A+: "volumes/raw"
        B+: "volumes/raw"
        C+: "volumes/raw"
      dtype: float32
#      sigma: 0.025

inference:
  path_checkpoint_trainer: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/stackedAffs_trainStack1b"
  crop_global_slice: # As compared to the input, how much should I crop (global crop)
    - [7,7]
    - [250, 250]
    - [250, 250]
  crop_prediction: # How much I crop the predicted tensor: (local_crop)
    - [1,1]
    - [10,10]
    - [10,10]



model:
  embeddingutils.models.unet.GeneralizedStackedPyramidUNet3D:
    loadfrom: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/stackedAffs_trainStack1b/checkpoint.pytorch"
    nb_stacked: 2 # Number of stacked models
    # If only model 1 is trained, then model 0 is run in inference mode. Atm there is no backprop between models:
    models_to_train: [1]
    downscale_and_crop:
      0: # Crop and/or downscale the output of model 0 before to give it as an input
        ds_factor: [1, 1, 1]
        #        crop_factor: [1, 3, 3]
        crop_slice: ":,6:-6,6:-6"
    # Parameters for each model in the stack:
    models_kwargs:
      global:
        add_embedding_heads: True
        depth: 4
        upsampling_mode: 'nearest'
        stop_decoder_at_depth: 0
        pyramid_fmaps: 128
        scale_factor:
          - [1, 3, 3]
          - [1, 2, 2]
          - [1, 3, 3]
          - [1, 2, 2]
      0:
        in_channels: 1
        output_fmaps: 12
        encoder_fmaps: [32, 128, 192, 384, 512, 512]
        res_blocks_3D:
          - [False, False]
          - [True, True, False, False]
          - [True, True, True, False]
          - [True, True]
          - [True, True]
        decoder_crops:
          1: "3:-3, 10:-10, 10:-10"
        nb_patch_nets: 1
        patchNet_kwargs:
          0:
            latent_variable_size: 128
            feature_maps: 16
            downscaling_factor: [1, 1, 1] # Inside patchNet, to reduce memory
            depth_level: 0
            patch_size: [7,19,19]
            patch_dws_fact: [1, 4, 4]
            patch_stride: [2, 36, 36]
            crop_targets: [[0,0], [80, 80 ], [80 ,80 ]]
            pred_dws_fact: [1, 3, 3]
            max_random_crop: [1, 25, 25]
            # Exclude a patch from training if the center does contain more than one gt label:
            central_shape: [1, 5, 5]
      1:
        in_channels: 13
        output_fmaps: 14
        encoder_fmaps: [128, 160, 256, 384, 512, 512]
        res_blocks_3D:
          - [False]
          - [True, False]
          - [True, False]
          - [True]
          - [True]
        decoder_crops:
          1: "3:-3, 14:-14, 14:-14"
        nb_patch_nets: 1
        patchNet_kwargs:
          global:
            central_shape: [1, 3, 3]
            latent_variable_size: 128
            feature_maps: 16
            downscaling_factor: [1, 1, 1] # Inside patchNet, to reduce memory
            downscale_and_crop_target:
              ds_factor: [1, 1, 1]
              crop_factor: [1, 2, 2]
              crop_slice: "3:-3, : , :"
          0:
            depth_level: 0
            patch_size: [7,19,19]
            patch_dws_fact: [1, 2, 2]
            patch_stride: [2, 18, 18]
            crop_targets: [[0,0], [30 ,30 ], [30 ,30 ]] # I don't need to crop as much as the prediction, because anyway there is the padding of the patches at the borders..
            pred_dws_fact: [1, 1, 1]
            max_random_crop: [1, 15, 15]

#trainer:
#  max_epochs: 10 # basically infinite
##  TODO: increase this if we need both affinity targets and directional DT targets
#  num_targets: 1
#
#  criterion:
#    # TODO: here you will be able to add the final losses (SoresenDice for affinities and L1 for DT)
#    losses: null
#    kwargs:
#      loss_type: "Dice" # "MSE"
#
#
#  metric:
#    evaluate_every: 'never'
#    quantizedVDT.metrics.ArandFromMWSDistances:
#      strides: null # [1,2,2]
#      z_direction: *z_direction
#      n_directions: *n_directions
#
#  optimizer:
#    Adam:
#      lr: 0.0001
#      weight_decay: 0.0005
##      betas: [0.9, 0.999]
#
#  intervals:
#    save_every: [1000, 'iterations']
#    validate_every:
#      frequency : [100, 'iterations']
#      for_num_iterations: 5
#
#  tensorboard:
#    log_scalars_every: [1, 'iterations']
#    log_images_every: [100, 'iterations']
#    log_histograms_every: 'never'
#    send_image_at_batch_indices: [0]
#    send_image_at_channel_indices: [0]
###    send_volume_at_z_indices: 'mid'
##    split_config_keys: True
##    log_anywhere: ['scalars']
#
#  callbacks:
##    gradients:
##      LogOutputGradients:
##        frequency: 1
#
#    essentials:
#      SaveAtBestValidationScore:
#        smoothness: 0
#        verbose: True
##      GradientClip:
##        clip_value: 1e-3
#      SaveModelCallback:
#        save_every: 500
#      PlotReconstructedCallback:
#        plot_every: 100
#
#    scheduling:
#      AutoLR:
#        monitor: 'validation_loss'
#        factor: 0.99
#        patience: '100 iterations'
#        monitor_while: 'validating'
#        monitor_momentum: 0.75
##        cooldown_duration: '50000 iterations'
#        consider_improvement_with_respect_to: 'previous'
#        verbose: True
#
#
#firelight:
#  affs_raw_gt:
#    RowVisualizer:
#      input_mapping:
#        global: [B: 0, D: 2]
#      pad_width: 1  # width of the border between images in pixels
#      pad_value: .2  # intensity of the border pixels
#      visualizers:
#        - IdentityVisualizer:
#            input: ['inputs', index: 2]
#            cmap: gray
#
#        - SegmentationVisualizer:
#            input: ['target', index: 2, C: 0]
#            #background_color: [0, 0, 0]
#            background_label: 0
#
#  affs:
#    ImageGridVisualizer:
#
#      input_mapping:
#        global: [B: 0, D: 2] # the mapping specified in 'global' is applied to all keys
#
#      pad_width: 1  # width of the border between images in pixels
#      pad_value: .2  # intensity of the border pixels
#      upsampling_factor: 1  # the whole grid is upsampled by this factor
#
#      row_specs: ['H', 'S', 'D', 'V',]
#      column_specs: ['W', 'B',  'C']
#
#      # Container visualizers always have the 'visualizers' argument. Its value has to be a list of visualizers
#      visualizers:
#
#        # visualize raw input
##        - IdentityVisualizer:
##            input: ['inputs', index: 2]
##            cmap: gray
##
##        - SegmentationVisualizer:
##            input: ['target', index: 2, C: 0]
##            #background_color: [0, 0, 0]
##            background_label: 0
#
##        - IdentityVisualizer:
##            input: ['prediction', index: 0, B: 0, D: "1:6", C: 0]
##            cmap: gray
#
#        - IdentityVisualizer:
#            input: ['target', index: 2, C: "1:12"]
#            cmap: gray
#
#        - IdentityVisualizer:
#            input: ['prediction', index: 0, C: ":11"]
#            cmap: gray
#
##
##        - PcaVisualizer:
##            input: ['prediction', index: 0, W: "::2", H: "::2"]
##
##        - PcaVisualizer:
##            input: ['prediction', index: 1]
##
##        - PcaVisualizer:
##            input: ['prediction', pre: {vaeAffs.utils.various.torch_tensor_zoom: {zoom: [1,1,1,2,2], order: 0}}, index: 2]
#
#
#
#

