

device: cuda

loaders:
  infer:
    inference_mode: True
    loader_config:
      # Number of processes to use for loading data. Set to (say) 10 if you wish to
      # use 10 CPU cores, or to 0 if you wish to use the same process for training and
      # data-loading (generally not recommended).
      batch_size: 1
      num_workers: 4
      drop_last: False
#      pin_memory: False
      shuffle: False
    name: C

    master_config:
    volume_config:
      # Sliding window size
      window_size: &shape [19, 405, 405]  # 288
#        A:
#        B: *shape
#        C: *shape
      # Sliding window stride
      stride: &stride [8, 46, 46] # This should be the same to the prediction, after being cropped
#        A:
#        B: *stride
#        C: *stride
      # Data slice to iterate over.
#      data_slice: '70:100,200:800,200:800'
      data_slice: ":16, 90:, 580: 1900"
#      data_slice: ':,:,:'
#        A: ':, :, :'
#        B: ':, :, :'
#        C: ':, :, :'

#      padding_mode: "constant"
      padding:
        - [6,6]
        - [193, 193]
        - [193, 193]
      # PATHS:
      path:
        A: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleA_train.h5'
        B: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleB_train.h5'
        C: '$HCI_HOME/datasets/cremi/SOA_affinities/sampleC_train.h5'
        B+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleB+_cropped_plus_mask.h5'
        A+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleA+_cropped_plus_mask.h5'
        C+: '$HCI_HOME/datasets/cremi/testvol/test_samples/sampleC+_cropped_plus_mask.h5'
      path_in_file:
        A: 'raw'
        B: 'raw_old'
        C: 'raw'
        A+: "volumes/raw"
        B+: "volumes/raw"
        C+: "volumes/raw"
      dtype: float32
#      sigma: 0.025

#FIXME: change this please...
inference:
#  path_checkpoint_trainer: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/refinedStackedModels_trainStack1c"
  crop_global_slice: # As compared to the input, how much should I crop (global crop)
    - [6,6]
    - [193, 193]
    - [193, 193]
  crop_prediction: # How much I crop the predicted tensor: (local_crop)
    - [0,0]
    - [10,10]
    - [10,10]
  return_patch_mask: False # Make sure to exclude the invalid affinities

model:
  vaeAffs.models.compute_IoU.IntersectOverUnionUNet:
    # EXTRA ARGS:
#    pre_crop_pred: ":, 33:-33, 33:-33"
#    stride: [1, 2, 2]
    number_patchNet: 1
    num_IoU_workers: 6
    offsets:
#       - [-1, 0, 0]
#       - [0, -2, 0]
#       - [0, 0, -2]
#       - [0, -4, 0]
#       - [0, 0, -4]
#       - [0, 0, -6]
#       - [0, -6, 0]
#       - [0, -4, -4]
#       - [0, -4, +4]
#       - [0, -8, 0]
#       - [0, 0, -8]
#       - [0, -8, -8]
#       - [0, -8, +8]
#       - [0, -10, 0]
       - [0, -10, 0]
       - [0, 0, -20]
#       - [-2, 0, 0]
#       - [-3, 0, 0]
    patch_size_per_offset: # They should be even (zero means that nothing is cropped)
      - [3, 0, 0]
      - [1, 0, 0]

    slicing_config:
      window_size: [3, 30, 30]
      stride: [3, 30, 30]

    loadfrom: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/fixedSingleModel_128_patch_2_19_19_b/checkpoint.pytorch"
    nb_stacked: 1 # Number of stacked models
    # If only model 1 is trained, then model 0 is run in inference mode. Atm there is no backprop between models:
    models_to_train: [0]
    detach_stacked_models: True

    #    downscale_and_crop:
    #      0: # Crop and/or downscale the output of model 0 before to give it as an input
    #        ds_factor: [1, 1, 1]
    ##        crop_factor: [1, 3, 3]
    #        crop_slice: ":,36:-36,36:-36"
    type_of_model: 'GeneralizedUNet3D'
    # Parameters for each model in the stack:
    models_kwargs:
      global:
        depth: 3
        upsampling_mode: 'nearest'
        stop_decoder_at_depth: 0
        pyramid_fmaps: 128
        output_fmaps: 128
        scale_factor:
          - [1, 3, 3]
          - [1, 3, 3]
          - [1, 3, 3]
        #          - [1, 2, 2]
        #          - [1, 2, 2]
        res_blocks_3D:
          - [False]
          - [True]
          - [True]
          - [True]
        #          - [True]
        #          - [True]
        res_blocks_decoder_3D:
          - [False]
          - [True]
          - [True]
          - [True]
        #          - [True]
        #          - [True]
        decoder_fmaps: [128, 128, 256, 512]
      0:
        in_channels: 1
        encoder_fmaps: [16, 48, 256]
        decoder_crops: # Crops AFTER the res_blocks at each level (at zero, we crop at the end)
          0: "1:-1, 3:-3, 3:-3"
          1: "2:-2, 3:-3, 3:-3"
          2: "2:-2, 3:-3, 3:-3"
          3: "1:-1, 3:-3, 3:-3"
        #          3: "3:-3, 3:-3, 3:-3"
        nb_patch_nets: 2
        patchNet_kwargs:
          global:
            latent_variable_size: 128
            feature_maps: 16
            downscaling_factor: [1, 1, 1] # Inside patchNet, to reduce memory
          #            downscale_and_crop_target:
          #              ds_factor: [1, 1, 1]
          #              crop_factor: [1, 1, 1]
          #              crop_slice: "2:-2, 100:-100, 100:-100"
          0:
            depth_level: 0
            patch_size: [3,19,19]
            patch_dws_fact: [1, 4, 4]
            patch_stride: [2, 36, 36]
            #            crop_targets: [[0,0], [0, 0], [0, 0]] # I don't need to crop as much as the prediction, because anyway there is the padding of the patches at the borders..
            pred_dws_fact: [1, 1, 1]
            #            max_random_crop: [0, 0, 0] # [1, 15, 15]
            max_random_crop: [1, 15, 15]
            # Exclude a patch from training if the center does contain more than one gt label:
            central_shape: [1, 3, 3]
          1:
            depth_level: 0
            central_shape: [1, 3, 3]
            patch_size: [3,19,19]
            patch_dws_fact: [1, 2, 2]
            patch_stride: [2, 18, 18]
            #            crop_targets: [[0,0], [19, 19], [19, 19]] # I don't need to crop as much as the prediction, because anyway there is the padding of the patches at the borders..
            pred_dws_fact: [1, 1, 1]
            max_random_crop: [1, 10, 10]



#autoencoder:
#  latent_variable_size: 128
#  path: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/VAE_27fc_24/model.pytorch"
#  path: "$HCI_HOME/pyCharm_projects/uppsala_hackathon/experiments/cremi/runs/Conv3DAutoEnc_Soresen_27_fc_noDownscale/model.pytorch"



trainer:
  max_epochs: 1000 # basically infinite
  num_targets: 1

  criterion:
    loss_name: "vaeAffs.models.compute_IoU.ComputeIoU"
    kwargs:
      loss_type: "Dice" # "MSE"
      stride: [1, 4, 4]
      offsets:
        - [0, 0, 16]
        - [0, 16, 0]
        - [0, 16, 16]
      ptch_kwargs:
        patch_size: [7,19,19]
        patch_dws_fact: [1, 2, 2]


  optimizer:
    Adam:
      lr: 0.0001
      weight_decay: 0.0005
      amsgrad: True
#      betas: [0.9, 0.999]

  intervals:
    save_every: [1000, 'iterations']
    validate_every:
      frequency : [100, 'iterations']
      for_num_iterations: 5

  tensorboard:
    log_scalars_every: [1, 'iterations']
    log_images_every: [300, 'iterations']
    log_histograms_every: 'never'
    send_image_at_batch_indices: [0]
    send_image_at_channel_indices: [0]
##    send_volume_at_z_indices: 'mid'
#    split_config_keys: True
#    log_anywhere: ['scalars']

  callbacks:
#    gradients:
#      LogOutputGradients:
#        frequency: 1

    essentials:
      SaveAtBestValidationScore:
        smoothness: 0
        verbose: True
#      GradientClip:
#        clip_value: 1e-3
      SaveModelCallback:
        save_every: 500
      PlotReconstructedCallback:
        plot_every: 100

    scheduling:
      AutoLR:
        monitor: 'validation_loss'
        factor: 0.99
        patience: '100 iterations'
        monitor_while: 'validating'
        monitor_momentum: 0.75
#        cooldown_duration: '50000 iterations'
        consider_improvement_with_respect_to: 'previous'
        verbose: True

##
#firelight:
#
#  debug_IoU:
#    ImageGridVisualizer:
#      input_mapping:
#        global: [B: "0", D: ":"]
#
#      pad_width: 1  # width of the border between images in pixels
#      pad_value: .2  # intensity of the border pixels
#      upsampling_factor: 5  # the whole grid is upsampled by this factor
#
#      row_specs: ['H', 'S', 'D', 'C', 'B']
#      column_specs: ['W', 'V']
#
#      # Container visualizers always have the 'visualizers' argument. Its value has to be a list of visualizers
#      visualizers:
#
#        - IdentityVisualizer:
#            input: ['IoU_patch1_cr']
#            cmap: gray
#            value_range: [0,1]
#        - IdentityVisualizer:
#            input: ['IoU_patch2_cr']
#            cmap: gray
#            value_range: [0,1]
#        - IdentityVisualizer:
#            input: ['IoU_score']
#            cmap: gray
#            value_range: [0,1]
#        - IdentityVisualizer:
#            input: ['IoU_score_pred']
#            cmap: gray
#            value_range: [0,1]
#
#  debug_IoU_2:
#    ImageGridVisualizer:
#      input_mapping:
#        global: [B: "0", D: ":"]
#
#      pad_width: 1  # width of the border between images in pixels
#      pad_value: .2  # intensity of the border pixels
#      upsampling_factor: 5  # the whole grid is upsampled by this factor
#
#      row_specs: ['H', 'S', 'D', 'C', 'B']
#      column_specs: ['W', 'V']
#
#      # Container visualizers always have the 'visualizers' argument. Its value has to be a list of visualizers
#      visualizers:
#
#        - IdentityVisualizer:
#            input: ['IoU_patch1']
#            cmap: gray
#            value_range: [0,1]
#        - IdentityVisualizer:
#            input: ['IoU_patch2']
#            cmap: gray
#            value_range: [0,1]
